---
title: "Test_MachineLearning_Loan"
author: "Claire Della Vedova"
date: "10 octobre 2017"
output: html_document
---

Selon <https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/>


```{r package}

install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
```

##1. Data

```{r data}
train<-read.csv("data/train_u6lujuX_CVtuZ9i.csv",stringsAsFactors = T)
str(train)

```


In this problem, we have to predict the Loan Status of a person based on his/ her profile.

##2.Pre-processing using Caret

```{r caret}

sum(is.na(train))

#Imputing missing values using KNN.Also centering and scaling numerical columns
preProcValues <- preProcess(train, method = c("knnImpute","center","scale"))

library('RANN')
train_processed <- predict(preProcValues, train) # met ensemble les variable dont on a imputé les missing values et les autres variables(catégorielle)

sum(is.na(train_processed))

#Converting outcome variable to numeric
train_processed$Loan_Status<-ifelse(train_processed$Loan_Status=='N',0,1)

id<-train_processed$Loan_ID
train_processed$Loan_ID <- NULL

#Checking the structure of processed train file
str(train_processed)


#Converting every categorical variable to numerical using dummy variables
dmy <- dummyVars(" ~ .", data = train_processed,fullRank = T)
train_transformed <- data.frame(predict(dmy, newdata = train_processed))

#Checking the structure of transformed train file
str(train_transformed)


#Converting the dependent variable back to categorical
train_transformed$Loan_Status<-as.factor(train_transformed$Loan_Status)
```



##3.Splitting data using Caret
```{r split}

#Spliting training set into two parts based on outcome: 75% and 25%
index <- createDataPartition(train_transformed$Loan_Status, p=0.75, list=FALSE)
trainSet <- train_transformed[ index,]
testSet <- train_transformed[-index,]




```

##4.Feature Selection using Caret

```{r feature selection}

#Feature selection using rfe in caret
control <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 3,
                   verbose = FALSE)



outcomeName<-'Loan_Status'

predictors<-names(trainSet)[!names(trainSet) %in% outcomeName]

Loan_Pred_Profile <- rfe(trainSet[,predictors], trainSet[,outcomeName],
                      rfeControl = control)
Loan_Pred_Profile


#Taking only the top 5 predictors
predictors<-c("Credit_History", "LoanAmount", "Loan_Amount_Term", "ApplicantIncome", "CoapplicantIncome")


```


##5.Training models using Caret

```{r}

model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm')
model_rf<-train(trainSet[,predictors],trainSet[,outcomeName],method='rf')
model_nnet<-train(trainSet[,predictors],trainSet[,outcomeName],method='nnet')
model_glm<-train(trainSet[,predictors],trainSet[,outcomeName],method='glm')

```

##5.Parameter tuning using Caret

```{r tuning}

fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5)


modelLookup(model='gbm')
modelLookup(model='rf')
modelLookup(model='nnet')
modelLookup(model='glm')


#Creating grid
grid <- expand.grid(n.trees=c(10,20,50,100,500,1000),shrinkage=c(0.01,0.05,0.1,0.5),n.minobsinnode = c(3,5,10),interaction.depth=c(1,5,10))

# training the model
model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneGrid=grid)

# summarizing the model
print(model_gbm)



plot(model_gbm)


#using tune length
model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneLength=10)
print(model_gbm)



```


##7.Variable Importance estimation using Caret
```{r variable importance}

varImp(object=model_gbm)
plot(varImp(object=model_gbm))

varImp(object=model_rf)
varImp(object=model_nnet)
varImp(object=model_glm)

```

##8.Prediction using Caret

```{r predictions}

predictions<-predict.train(object=model_gbm,testSet[,predictors],type="raw")
table(predictions)
confusionMatrix(predictions,testSet[,outcomeName])
```




